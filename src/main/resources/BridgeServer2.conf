bridge.env=local
bridge.user=your-username-here

# CloudFormation did not match our original environment tokens, so we must map between them in some cases
local.bucket.suffix = local-${bridge.user}
dev.bucket.suffix = develop
uat.bucket.suffix = uat
prod.bucket.suffix = prod

aws.account.id = 381492192635

heartbeat.interval.minutes=30

channel.throttle.max.requests = 1
channel.throttle.timeout.seconds = 300

ses.notification.topic.arn = arn:aws:sns:us-east-1:381492192635:SNSBounces-develop

synapse.user = yours-synapse-user
synapse.access.token = your-synapse-access-token

synapse.aws.account.id = 325565585839

synapse.endpoint = https://repo-prod.prod.sagebase.org/

exporter.synapse.user = your-exporter-synapse-user
exporter.synapse.access.token = your-exporter-synapse-access-token

exporter.synapse.id = 3509740
test.synapse.user.id = 3509740

virus.scan.trigger.topic = virus-scan-trigger-${bucket.suffix}
virus.scan.result.topic = virus-scan-result-${bucket.suffix}
virus.scan.result.sqs.queue = virus-scan-result-${bucket.suffix}

# Excludes the original try. For example, if this is set to 1, DDB will try a total of twice (one try, one retry)
ddb.max.retries = 1

# Hibernate (MySQL) configs
hibernate.connection.password = your password here
hibernate.connection.url = jdbc:mysql://localhost:3306/your-db-name-here
hibernate.connection.username = your username here
hibernate.connection.useSSL = false

# Max number of connections under our current plan is 256
redis.max.total = 50
redis.min.idle = 3
redis.max.idle = 50
redis.timeout = 2000

elasticache.url = redis://localhost:6379

async.worker.thread.count = 20

support.email.plain = biaffect-bridge-ops@googlegroups.com
support.email = Bridge (Biaffect) <${support.email.plain}>
sysops.email = Bridge IT <biaffect-bridge-ops@googlegroups.com>

email.unsubscribe.token = dummy-value

bridge.healthcode.redis.key = zEjhUL/FVsN8vti6HO27XgrM32i1a3huEuXWD4Hq06I=

use.https.forwarding = true

fphs.id.add.limit = 10
uat.fphs.id.add.limit = 100
prod.fphs.id.add.limit = 100

external.id.add.limit = 10
uat.external.id.add.limit = 100
prod.external.id.add.limit = 100

local.host.postfix = -local-${bridge.user}.biaffectbridge.com
dev.host.postfix = -develop.biaffectbridge.com
uat.host.postfix = -staging.biaffectbridge.com
prod.host.postfix = .biaffectbridge.com

webservices.url = https://ws${host.postfix}
local.webservices.url = http://localhost:9000

# Synapse accounts, used by Bridge when creating Synapse projects.
admin.synapse.id = 3509740
downstream.etl.synapse.id = 3509740

# Synapse Team IDs, used by the BridgeStudyCreator when creating Synapse projects.
team.bridge.admin = 3510447
team.bridge.staff = 3510446

# Synapse mHealth Project Tracking Views
synapse.tracking.view =

# Upload buckets
upload.bucket = org-biaffectbridge-upload-${bucket.suffix}

# Health Data Attachment buckets
attachment.bucket = org-biaffectbridge-attachment-${bucket.suffix}

# Exporter 3 Health Data buckets
health.data.bucket.raw = org-biaffectbridge-rawhealthdata-${bucket.suffix}

# Backfill buckets
local.backfill.bucket = org-biaffectbridge-backfill-local
dev.backfill.bucket = org-biaffectbridge-backfill-develop
uat.backfill.bucket = org-biaffectbridge-backfill-staging
prod.backfill.bucket = org-biaffectbridge-backfill-prod

# Upload CMS certificate information
upload.cms.certificate.country = US
upload.cms.certificate.state = WA
upload.cms.certificate.city = Seattle
upload.cms.certificate.organization = Biaffect
upload.cms.certificate.team = Bridge
upload.cms.certificate.email = biaffect-bridge-ops@googlegroups.com

# Buckets for CMS key pairs
# System needs the CMS keys in the shared local buckets, or integration tests fail
upload.cms.cert.bucket = org-biaffectbridge-upload-cms-cert-${bucket.suffix}
local.upload.cms.cert.bucket = org-biaffectbridge-upload-cms-cert-local
upload.cms.priv.bucket = org-biaffectbridge-upload-cms-priv-${bucket.suffix}
local.upload.cms.priv.bucket = org-biaffectbridge-upload-cms-priv-local

# Maximum 100 MB per zip entry
max.zip.entry.size = 100000000
# Maximum 100 zip entries per archive
max.num.zip.entries = 100

# Buckets for the content of each consent revision
consents.bucket = org-biaffectbridge-consents-${bucket.suffix}

# SQS queues
# For older queues, we specify each env separately, because the older naming conventions were different.

# Bridge Exporter SQS queues
local.exporter.request.sqs.queue=Bridge-EX-Request-${bucket.suffix}
dev.exporter.request.sqs.queue=Bridge-EX-Request-develop
uat.exporter.request.sqs.queue=Bridge-EX-Request-staging
prod.exporter.request.sqs.queue=Bridge-EX-Request-prod

# S3 Event Notification Callback (Upload Auto-Complete)
local.s3.notification.sqs.queue=Bridge-UploadComplete-Notification-${bucket.suffix}
dev.s3.notification.sqs.queue=Bridge-UploadComplete-Notification-develop
uat.s3.notification.sqs.queue=Bridge-UploadComplete-Notification-staging
prod.s3.notification.sqs.queue=Bridge-UploadComplete-Notification-prod

#Bridge Participant Roster Download Worker SQS queues
local.workerPlatform.request.sqs.queue=Bridge-WorkerPlatform-Request-${bucket.suffix}
dev.workerPlatform.request.sqs.queue=Bridge-WorkerPlatform-Request-develop
uat.workerPlatform.request.sqs.queue=Bridge-WorkerPlatform-Request-staging
prod.workerPlatform.request.sqs.queue=Bridge-WorkerPlatform-Request-prod

# Bridge Integration Test queues
integ.test.sqs.queue=Bridge-IntegTest-${bucket.suffix}

# SQS Dead Letter Queue
dead.letter.queue.arn=arn:aws:sqs:us-east-1:381492192635:Bridge-Dead-Letter-Queue-Develop
prod.dead.letter.queue.arn=arn:aws:sqs:us-east-1:381492192635:Bridge-Dead-Letter-Queue

# List of apps that should never be deleted
app.whitelist = api,api-2,shared

usersigned.consents.bucket = org-biaffectbridge-usersigned-consents-${bucket.suffix}

# Bootstrap user for integration tests (will only be used when first initializing your database).
# This should match the synapse.test.user.* credentials set for your integration test properties 
# in your bridge-sdk-test.properties file.
admin.email = dummy-value
admin.password = dummy-value
admin.synapse.user.id = 3509740

synapse.oauth.url = https://repo-dev.dev.sagebase.org/auth/v1/oauth2/token
prod.synapse.oauth.url = https://repo-prod.prod.sagebase.org/auth/v1/oauth2/token
synapse.oauth.client.id = dummy-value
synapse.oauth.client.secret = dummy-value

# To reverse geocode location of appointments in CRC controller
crc.geocode.api.key = dummy-value

cuimc.test.location.url = https://xeperno.nyp.org/nypcovi/Location/_search
cuimc.test.username = dummy-value
cuimc.test.password = dummy-value

# These are not currently the production endpoint.
cuimc.prod.location.url = https://xeperno.nyp.org/nypcovi/Location/_search
cuimc.prod.username = dummy-value
cuimc.prod.password = dummy-value

gbf.order.place.url = https://www.gbfmedical.com/oap/api/order
gbf.order.status.url = https://www.gbfmedical.com/oap/api/status
gbf.order.cancel.url = https://www.gbfmedical.com/oap/api/cancelorder
gbf.order.returns.url = https://www.gbfmedical.com/oap/api/returns
gbf.ship.confirmation.url = https://www.gbfmedical.com/oap/api/confirm
gbf.api.key = dummy-value

# Schedule timeline metadata records batch persist. Values above 100 do not seem to 
# improve performance, but values under 100 start to degrade it a bit.
schedule.batch.size = 100

# The allowlist of URL query parameters.
# Other parameters in the query will not show up in the server log,
# in order to protect PII.
query.param.allowlist = type,appId,studyId,IdFilter,assignmentFilter,externalId,identifier,ownerId,newIdentifier,name,notes,tags,includeDeleted,physical,format,summary,startTime,endTime,pageSize,offsetKey,offsetBy,tag,category,minRevision,maxRevision,queryParam,createAccount,createdOnStart,createdOnEnd,consents,scheduledOnStart,scheduledOnEnd,startDate,endDate,deleteReauthToken,until,daysAhead,minimumPerSchedule,mostRecent,mostrecent,published,newSchemaRev,synchronous,redrive

# Participant File S3 bucket name
participant-file.bucket = org-biaffectbridge-participantfile-${bucket.suffix}

# Create Participant rate limiting production constants (3 every 5 minutes)
prod.create-participant.rate-limiter.initial-count = 3
prod.create-participant.rate-limiter.maximum-count = 3
prod.create-participant.rate-limiter.refill-interval-seconds = 100
prod.create-participant.rate-limiter.refill-count = 1

# Create Participant rate limiting non-production constants (10 per second with a max of 100)
create-participant.rate-limiter.initial-count = 100
create-participant.rate-limiter.maximum-count = 100
create-participant.rate-limiter.refill-interval-seconds = 1
create-participant.rate-limiter.refill-count = 10

# Participant file rate limiting production constants
# 1 MB
prod.participant-file.rate-limiter.initial-bytes = 1000000
# 10 MB
prod.participant-file.rate-limiter.maximum-bytes = 10000000
# 1 hr
prod.participant-file.rate-limiter.refill-interval-seconds = 3600
# 1 MB
prod.participant-file.rate-limiter.refill-bytes = 1000000

# Participant file rate limiting test constants
# 1 KB
participant-file.rate-limiter.initial-bytes = 1000
# 1 KB
participant-file.rate-limiter.maximum-bytes = 1000
# 5 s
participant-file.rate-limiter.refill-interval-seconds = 5
# 1 KB
participant-file.rate-limiter.refill-bytes = 1000

# Buckets for the participant roster worker to upload a completed zip file
participantroster.bucket = org-biaffectbridge-participantroster-${bucket.suffix}

# Public documents folder
docs.bucket = docs${host.postfix}

# In production, the bucket name and domain name are the same because we can configure 
# the domain name in Route 53. In local, development, and staging environments, we do 
# not (can not?) configure DNS entries because they are in a separate AWS account. 
# So we use the full S3 website hosting URL.
docs.website.url = ${docs.bucket}
local.docs.website.url = docs-${bridge.user}.biaffectbridge.org.s3-website-us-east-1.amazonaws.com
dev.docs.website.url = docs-develop.biaffectbridge.org.s3-website-us-east-1.amazonaws.com
